{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/toby-htx/ONNX-Sharing-Session/blob/main/BiLSTM_in_PyTorch_W2V.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbbHfa4sI9KP"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v = api.load(\"word2vec-google-news-300\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9l5y_ynSy1J"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/toby-htx/Onnx-Sharing-Session.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5891LrII_jB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import re\n",
        "\n",
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L0GVNS_JCqI"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('./Onnx-Sharing-Session/Data/Isear(Fear&Joy).csv')\n",
        "df = df[['Emotion','Statement']]\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['Emotion'] = le.fit_transform(df['Emotion'])\n",
        "\n",
        "X = df['preprocessedStatement'].tolist()\n",
        "Y = df.pop('Emotion').tolist()\n",
        "\n",
        "train_iter = (zip(Y,X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h549OVrJExo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmVdH7DUJH9A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "emb_dim=300\n",
        "vocab_size = vocab.__len__()\n",
        "weights_matrix = np.zeros((vocab_size, emb_dim))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(vocab.get_itos()):\n",
        "    try: \n",
        "        weights_matrix[i] = w2v[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        pass\n",
        "        # weights_matrix[i] = np.zeros((1, emb_dim))\n",
        "        # weights_matrix[i] = np.random.rand(1, emb_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uGKhL60JIwb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test= train_test_split(X,Y,test_size=0.05,stratify = Y) \n",
        "\n",
        "x_val, y_val = x_train[:100], y_train[:100] \n",
        "x_train, y_train = x_train[100:], y_train[100:]\n",
        "\n",
        "train_data = list(zip(y_train,x_train))\n",
        "valid_data = list(zip(y_val,x_val))\n",
        "test_data = list(zip(y_test,x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdB2GhjZJKuD"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZZCsbaAJMgf"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "\n",
        "    label_list, text_list, text_len  = [], [], []\n",
        "   \n",
        "    for (_label,_text) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        text_len.append(len(processed_text))\n",
        "   \n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    text_len = torch.tensor(text_len, dtype=torch.int64)\n",
        "   \n",
        "    text_list_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
        "   \n",
        "    return label_list, text_list_padded, text_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSvnL5NFJX0T"
      },
      "outputs": [],
      "source": [
        "class LSTM_W2V(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, weights) :\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(weights))\n",
        "        self.embeddings.weight.requires_grad = False ## freeze embedding\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.maxpool = nn.MaxPool1d(1)\n",
        "        self.avgpool = nn.AvgPool1d(1)\n",
        "        self.linear = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
        "        self.linear2 = nn.Linear(hidden_dim*2, 2)\n",
        "\n",
        "    def forward(self, x, text_len):\n",
        "        \n",
        "        h0 = torch.zeros(2, x.size(0), self.hidden_dim)\n",
        "        c0 = torch.zeros(2, x.size(0), self.hidden_dim)\n",
        "  \n",
        "        x = self.embeddings(x)\n",
        "        packed_embedded = pack_padded_sequence(input=x, lengths=text_len, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, (ht, ct) = self.lstm(packed_embedded, (h0,c0))\n",
        "        lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        out_max_pool=self.maxpool(lstm_out)\n",
        "        out_avg_pool=self.avgpool(lstm_out)\n",
        "\n",
        "        out = torch.cat((out_max_pool, out_avg_pool), 1)\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        out = F.relu(self.linear(out))\n",
        "        preds = self.linear2(out)\n",
        "            \n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4IlfNfeJkYc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#embedding_dim=300\n",
        "hidden_dim=32\n",
        "\n",
        "model = LSTM_W2V(vocab_size, emb_dim, hidden_dim, weights_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox91Tyb9JnVA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dl = DataLoader(valid_data, batch_size=BATCH_SIZE,collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_data, batch_size=1,collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qxvp3xfeJsc_"
      },
      "outputs": [],
      "source": [
        "def train_model(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "        \n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for y, x, len in train_dl:\n",
        "            y = y.long()\n",
        "            x = x.long()\n",
        "            y_pred = model(x, len)\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc = validation_metrics(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
        "        \n",
        "def validation_metrics(model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    for y, x, len in valid_dl:\n",
        "        y = y.long()\n",
        "        x = x.long()\n",
        "        y_hat = model(x, len)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "    return sum_loss/total, correct/total\n",
        "\n",
        "def predict_test_cases(model, test_dl):\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, x, len in test_dl:\n",
        "            x = x.long()\n",
        "            y_hat = model(x, len)\n",
        "            pred = torch.max(y_hat, 1)[1]\n",
        "            pred_list.append(pred)\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIEYy8gIJ6C1",
        "outputId": "44f19ffa-ae7d-4e80-8591-ef8f267a8314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss 0.693, val loss 0.694, val accuracy 0.480\n",
            "train loss 0.687, val loss 0.696, val accuracy 0.480\n",
            "train loss 0.684, val loss 0.696, val accuracy 0.480\n",
            "train loss 0.687, val loss 0.694, val accuracy 0.480\n",
            "train loss 0.683, val loss 0.699, val accuracy 0.480\n",
            "train loss 0.683, val loss 0.698, val accuracy 0.480\n"
          ]
        }
      ],
      "source": [
        "train_model(model, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKR50v2sJ8wi"
      },
      "outputs": [],
      "source": [
        "pred_list = predict_test_cases(model, test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUXH669nJ_RM",
        "outputId": "414e5f1b-f3a4-4c9b-a7ee-1338291c2531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.80      0.78        54\n",
            "           1       0.79      0.76      0.78        55\n",
            "\n",
            "    accuracy                           0.78       109\n",
            "   macro avg       0.78      0.78      0.78       109\n",
            "weighted avg       0.78      0.78      0.78       109\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FhBjg2_KBqS"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'saved_weights.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BiLSTM in PyTorch(W2V).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "4dc95141b56626e47ddbcd300f72513f83f102d2dd66bd99e6fee4a1e017e167"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('tf2.4': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
