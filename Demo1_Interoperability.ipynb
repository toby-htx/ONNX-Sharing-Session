{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/toby-htx/ONNX-Sharing-Session/blob/main/Demo1_Interoperability.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBIcPkyna4C"
      },
      "source": [
        "# **Demo 1: Interoperability**\n",
        "\n",
        "**PyTorch -> ONNX -> Tensorflow**\n",
        "\n",
        "In this demo, we are going to convert a model written in the PyTorch framework to ONNX format, then convert the new ONNX model into a Tensorflow model. Specifically, our Tensorflow BiLSTM model that takes in Google's Word2Vec embeddings as input has been rewritten with the PyTorch framework, and had been used for the Fine Grained Sentiment Analysis workstream. We will convert this model into an ONNX model, and subsequently a Tensorflow model.\n",
        "\n",
        "You will need to change the **Runtime** to have a **TPU hardware accelerator**, then select '**Run All**'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pa57z83CVw8"
      },
      "source": [
        "##Secton 1: PyTorch Model##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MypizZnW0ezv"
      },
      "source": [
        "1) We need to **import the Word2Vec embeddings**. This will take around **20 minutes**  as it is huge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToJkpsERmjxR",
        "outputId": "8dd321a8-8e5d-478a-ecbf-7fbbfad3a1d8"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v = api.load(\"word2vec-google-news-300\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2TZYFh15Hf"
      },
      "source": [
        "2) Import the dataset and preprocess it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/toby-htx/Onnx-Sharing-Session.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "CyeAUtRXmnlT",
        "outputId": "1915d576-a3ab-4657-a9cd-bc2773b5d24a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import re\n",
        "\n",
        "def process_text(document):\n",
        "     \n",
        "    # Remove extra white space from text\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "         \n",
        "    # Remove all the special characters from text\n",
        "    document = re.sub(r'\\W', ' ', str(document))\n",
        " \n",
        "    return document\n",
        "\n",
        "df = pd.read_csv('./Onnx-Sharing-Session/Data/Isear(Fear&Joy).csv')\n",
        "df = df[['Emotion','Statement']]\n",
        "df['preprocessedStatement'] = df.Statement.apply(process_text)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['Emotion'] = le.fit_transform(df['Emotion'])\n",
        "\n",
        "X = df['preprocessedStatement'].tolist()\n",
        "Y = df.pop('Emotion').tolist()\n",
        "\n",
        "list(le.inverse_transform([0,1]))\n",
        "\n",
        "train_iter = (zip(Y,X))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUlHlgIW1Wrc"
      },
      "source": [
        "3) We have to build the **Vocab** and **embedding matrix** using the **PyTorch** way.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npLN642Wm4FA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6OBqIXfm64M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_size = vocab.__len__()\n",
        "weights_matrix = np.zeros((vocab_size, 300))\n",
        "words_found = 0\n",
        "\n",
        "for i, word in enumerate(vocab.get_itos()):\n",
        "    try: \n",
        "        weights_matrix[i] = w2v[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        pass\n",
        "        # weights_matrix[i] = np.zeros((1, 300))\n",
        "        # weights_matrix[i] = np.random.rand(1, emb_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Jmf0cP2EAS"
      },
      "source": [
        "4) Split the data into training, validation, and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AG5r8k6nXzC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test= train_test_split(X,Y,test_size=0.05,stratify = Y)\n",
        "\n",
        "x_val, y_val = x_train[:100], y_train[:100] \n",
        "x_train, y_train = x_train[100:], y_train[100:]\n",
        "\n",
        "train_data = list(zip(y_train,x_train))\n",
        "valid_data = list(zip(y_val,x_val))\n",
        "test_data = list(zip(y_test,x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HxpplXnnkc-"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ4Q4eKj2a5o"
      },
      "source": [
        "5) The datasets should be loaded using **PyTorch's DataLoader**, and prepared with a **collate_fn**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTGuKyrAuFZQ"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "\n",
        "    max_len =131 #Diff from PyTorch notebook\n",
        "\n",
        "    #Diff from PyTorch notebook: label_list, text_list, text_len  = [], [], []\n",
        "    label_list, text_list  = [], []\n",
        "   \n",
        "    for (_label,_text) in batch:\n",
        "        label_list.append(_label)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        #Diff from PyTorch notebook: text_len.append(len(processed_text))\n",
        "   \n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "\n",
        "    #Diff from PyTorch notebook: text_len = torch.tensor(text_len, dtype=torch.int64)\n",
        "\n",
        "    text_list[0] = nn.ConstantPad1d((0, max_len - text_list[0].shape[0]), 0)(text_list[0])\n",
        "   \n",
        "    text_list_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
        "   \n",
        "    return label_list, text_list_padded #Diff from PyTorch notebook: , text_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl7DrX7z2w4v"
      },
      "source": [
        "6) Prepare the BiLSTM model architecture with the PyTorch framework. \n",
        "The model architecture was inspired by that used in *Z. Hameed and B. Garcia-Zapirain, \"Sentiment classification using a single-layered BiLSTM model\", IEEE Access, vol. 8, pp. 73992-74001, 2020.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v60v9kCyue-z"
      },
      "outputs": [],
      "source": [
        "class LSTM_W2V(torch.nn.Module) :\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, weights) :\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.embeddings.weight.data.copy_(torch.from_numpy(weights))\n",
        "        self.embeddings.weight.requires_grad = False\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.maxpool = nn.MaxPool1d(1)\n",
        "        self.avgpool = nn.AvgPool1d(1)\n",
        "        self.linear = nn.Linear(hidden_dim*2, hidden_dim*2)\n",
        "        self.linear2 = nn.Linear(hidden_dim*2, 2)\n",
        "\n",
        "    def forward(self, x): #Diff from PyTorch notebook: text_len\n",
        "        \n",
        "        h0 = torch.zeros(2, x.size(0), self.hidden_dim)\n",
        "        c0 = torch.zeros(2, x.size(0), self.hidden_dim)\n",
        "\n",
        "        x = self.embeddings(x)\n",
        "        #Diff from PyTorch notebook: packed_embedded = pack_padded_sequence(input=x, lengths=text_len, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, (ht, ct) = self.lstm(x, (h0,c0)) # Diff from PyTorch notebook: packed_embedded\n",
        "        #Diff from PyTorch notebook: lstm_out, output_lengths = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        out_max_pool=self.maxpool(lstm_out)\n",
        "        out_avg_pool=self.avgpool(lstm_out)\n",
        "\n",
        "        out = torch.cat((out_max_pool, out_avg_pool), 1)\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        out = F.relu(self.linear(out))\n",
        "        preds = self.linear2(out)\n",
        "            \n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhE3kv3P3rtN"
      },
      "source": [
        "7) Instantiate the BiLSTM model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMco3vnuvvgM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "embedding_dim=300\n",
        "hidden_dim=32\n",
        "\n",
        "model = LSTM_W2V(vocab_size, embedding_dim, hidden_dim, weights_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR0FDghq3x7e"
      },
      "source": [
        "8) Prepare **DataLoaders** to pass datasets into the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqL-INTGxR10"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dl = DataLoader(valid_data, batch_size=BATCH_SIZE,collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_data, batch_size=1,collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuDgVWev38cD"
      },
      "source": [
        "9) Prepare functions to train, evaluate, and test the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pr_7RJJtxbZ2"
      },
      "outputs": [],
      "source": [
        "def train_model(model, epochs=10, lr=0.001):\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
        "        \n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "        sum_loss = 0.0\n",
        "        total = 0\n",
        "        for y, x in train_dl: #Diff from PyTorch notebook: len\n",
        "            y = y.long()\n",
        "            x = x.long()\n",
        "            y_pred = model(x) #Diff from PyTorch notebook: len\n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            sum_loss += loss.item()*y.shape[0]\n",
        "            total += y.shape[0]\n",
        "        val_loss, val_acc = validation_metrics(model, val_dl)\n",
        "        if i % 5 == 1:\n",
        "            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
        "        \n",
        "def validation_metrics(model, valid_dl):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    sum_loss = 0.0\n",
        "    for y, x in valid_dl: #Diff from PyTorch notebook: len\n",
        "        y = y.long()\n",
        "        x = x.long()\n",
        "        y_hat = model(x) #Diff from PyTorch notebook: len\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        pred = torch.max(y_hat, 1)[1]\n",
        "        correct += (pred == y).float().sum()\n",
        "        total += y.shape[0]\n",
        "        sum_loss += loss.item()*y.shape[0]\n",
        "    return sum_loss/total, correct/total\n",
        "\n",
        "def predict_test_cases(model, test_dl):\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for _, x in test_dl: #Diff from PyTorch notebook: len\n",
        "            x = x.long()\n",
        "            y_hat = model(x) #Diff from PyTorch notebook: len\n",
        "            pred = torch.max(y_hat, 1)[1]\n",
        "            pred_list.append(pred)\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFn3U61w4P7a"
      },
      "source": [
        "10) Train the model. Note that model performance will not be ideal as we are trying to train it as fast as we can, hence the number of epochs is only 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNGgox2QxsBS"
      },
      "outputs": [],
      "source": [
        "train_model(model, epochs=2, lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5a3KN-e4UAE"
      },
      "source": [
        "11) Test the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ6FS_EeE8YV"
      },
      "outputs": [],
      "source": [
        "pred_list = predict_test_cases(model, test_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po00FIemCWs4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, pred_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGiwv6Qf4gu-"
      },
      "source": [
        "12) Save the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2zd3WaDxzGS"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'saved_weights.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLttUlBqDPh_"
      },
      "source": [
        "##Secton 2: ONNX Model##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXfTwpR25AZI"
      },
      "source": [
        "13) Export the PyTorch model as an **ONNX model**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hml9GkBNx3FS"
      },
      "outputs": [],
      "source": [
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI3XPBl4dbKR"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.rand((1,131), requires_grad=True).long()\n",
        "\n",
        "torch.onnx.export(model,                     # model being run\n",
        "                  dummy_input,               # model input (or a tuple for multiple inputs)\n",
        "                  \"model_trial.onnx\",        # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  verbose =True\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YshMLlUg48kr"
      },
      "source": [
        "14) Download the required packages to use **ONNX** and to convert an ONNX model to **Tensorflow**. Note that some Tensorflow versions might not be compatible. This is a disadvantage of using ONNX: **you need to make sure the versions between ONNX and the DL frameworks are compatible**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znsazMNELf-9"
      },
      "outputs": [],
      "source": [
        "!pip install onnx onnx-tf onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Hw3lzNDVm7"
      },
      "source": [
        "##Secton 3: Tensorflow Model##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Ujp0k25--u"
      },
      "source": [
        "15) Convert the ONNX model into a Tensorflow model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZhThSohqaL4"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "model_onnx = onnx.load('./model_trial.onnx')\n",
        "\n",
        "tf_rep = prepare(model_onnx)\n",
        "\n",
        "#Export tensorflow model as .pb file\n",
        "tf_rep.export_graph('./model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k2M0TdB6ZeN"
      },
      "source": [
        "16) Use **Tensorflow framework/code** to prepare your inputs to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpeEbOjcMOla"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def tf_prepare(text):\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(text)\n",
        "    encoded_text = t.texts_to_sequences(text)\n",
        "    x = pad_sequences(encoded_text, maxlen=131, padding='post')\n",
        "    x = x.astype(np.int64)\n",
        "    x_tf =tf.convert_to_tensor(x)\n",
        "    return x_tf\n",
        "\n",
        "text = ['I love shiba inus']\n",
        "\n",
        "x_tf = tf_prepare(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Uq_Ekv6kJS"
      },
      "source": [
        "17) Run the **Tensorflow** model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoMnRBvcu8B1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time_tf = time.time()\n",
        "tf_outputs = tf_rep.run(x_tf)._0\n",
        "print(\"Time taken by Tensorflow model: \", time.time() - start_time_tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN893q89eVgs"
      },
      "outputs": [],
      "source": [
        "tf_outputs_clean = np.argmax(tf_outputs, 1)\n",
        "print(tf_outputs_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26Drs-dYDbco"
      },
      "source": [
        "##Extra: ONNX Model Test##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLWZagfi6zS-"
      },
      "source": [
        "18) Run the **ONNX** model and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHuhzD2M0c0Y"
      },
      "outputs": [],
      "source": [
        "#test if ONNX conversion worked\n",
        "import onnxruntime as rt\n",
        "\n",
        "text = ['I love shiba inus']\n",
        "\n",
        "def onnx_prepare(text):\n",
        "    t = Tokenizer()\n",
        "    t.fit_on_texts(text)\n",
        "    encoded_text = t.texts_to_sequences(text)\n",
        "    x = pad_sequences(encoded_text, maxlen=131, padding='post')\n",
        "    x = x.astype(np.int64)\n",
        "    return x\n",
        "\n",
        "x_onnx = onnx_prepare(text)\n",
        "\n",
        "model = ('model_trial.onnx')\n",
        "start_time = time.time()\n",
        "session = rt.InferenceSession(model)\n",
        "input_name = session.get_inputs()[0].name\n",
        "label_name = session.get_outputs()[0].name\n",
        "onnx_predictions = session.run([label_name], {input_name: x_onnx})[0]\n",
        "\n",
        "onnx_pred_clean = np.argmax(onnx_predictions, 1)\n",
        "\n",
        "print(onnx_pred_clean)\n",
        "print(\"Time taken by ONNX model: \", time.time() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQKMXYqh7lXJ"
      },
      "source": [
        "**Question**: Why not just import the ONNX model in Demo 2 and convert it into PyTorch?\n",
        "\n",
        "**Answer**: There is a library called onnx2pytorch, but it is full of bugs.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Demo1: Interoperability.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
